import numpy as np
#import loglikelihood as li
import matplotlib.pyplot as plt
#import parabolic_min as minim
#import univariate as uni
#import QuasiNewton as qn

delta_m2 = 2.4 
l_0 = 295
    
def getgrad(func, inputs):
    '''
    Utilises central different scheme to returns approx of grad of function.
    
    Args:
        func:: function
            Function to differentiate.
        inputs:: list
            list of values at which the grad is to be evaluated.
    Returns:
        gradf:: np.ndarry
            grad of f evaluated at inputs.
    '''
    gradf = np.zeros((len(inputs)))
    p1 = inputs.copy()
    p2 = inputs.copy()
    for i in np.arange(len(inputs)):
        delta = p2[i] * 1e-4
        p2[i] = p2[i] + delta
        p1[i] = p1[i] - delta 
        
        fplus = func(*p2)
        fminus = func(*p1)
        
        fprime = (fplus - fminus) / (2 * delta)
        gradf[i] = fprime
        p1 = inputs.copy()
        p2 = inputs.copy()
        
    return gradf


def grad_min(func, p0):
    '''
    Args:
        func:: function
            Function to be minimised.
        p0:: list
            The initial guesses for the parameters of the function
            , has form of [x0, x1 ... xn] for n paramters.
    Returns:
        params:: np.ndarry
            Minimised parameters for function.
        fi:: float
            Minimised function value.     
    '''
    params = p0.copy()
    alpha = 1e-4
    count = 1
    p_chain = [params]
        
    params = p0.copy()

    p_ratio = np.ones(len(p0)) # Array of fractional change of each parameter between iterations  
    cond = np.ones(len(p0)) * 0.00001 # Iteration criteria.

    while np.all(np.greater(cond, p_ratio)) == False: 
        if count < 100000:
            p_old = params.copy()
            
            dp = np.reshape(getgrad(func, params), (len(p0),))   
            dp = dp.tolist()
    
            p_new = np.zeros(len(p0),)   # Iterating new parameters (grad method)
            for i in np.arange(len(dp)):
                p_new[i] = params[i] - dp[i] * alpha     
    
            fi = func(*p_new) # Function value at this value
            
            p_chain.append(params)
            params = p_new  # Redefining f0 with the new parameters to iterate again
            
            p_ratio = np.ones(len(p0))  # Updating p_ratio with iterated parameters
            for i in range(len(p0)):
                p_ratio[i] = abs((p_new[i] - p_old[i])/p_old[i])
            
            count+=1    
            print(params)
        else:
            break
        
    if count < 100000:
        return params, fi, p_chain
    
    else:
        raise Exception('Grad method does not converge for given parameters')
